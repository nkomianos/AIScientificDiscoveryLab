# Kosmos Validation Roadmap

**End Goal:** Validate implementation against Lu et al. (2024) paper claims

**Paper Claims:**
- 79.4% accuracy on scientific statements
- 7 validated discoveries
- 20 research cycles with 10 tasks per cycle
- Autonomous hypothesis generation, experiment design, execution, and interpretation

**Reality Check:** The paper omits critical details (benchmark dataset, accuracy methodology, discovery criteria). Full reproduction may not be possible. The goal is honest measurement against reasonable interpretations of these claims.

---

## Critical Path Dependencies

```
Code Generator → Safe Execution → Result Interpretation → Multi-Cycle Runs
      ↓               ↓                    ↓                     ↓
   Phase 2.1      Phase 2.2            Phase 2.3              Phase 3
```

The skipped tests are not optional - they represent untested critical path components.

---

## Phase 1: Component Coverage (Complete)

**Status:** Done

- [x] Hypothesis generation via LLM
- [x] Experiment protocol design
- [x] Result analysis and interpretation
- [x] Multi-provider LLM support (Anthropic, OpenAI, LiteLLM/Ollama)
- [x] Research workflow state machine
- [x] Convergence detection
- [x] 32/39 E2E tests passing

---

## Phase 2: Critical Path Completion

**Status:** Not Started

**Objective:** Fix skipped tests that block the autonomous research loop.

### 2.1 Code Generation Path
- [ ] Unskip test_experiment_designer (stale skip reason)
- [ ] Fix test_code_generator (needs proper ExperimentProtocol setup)
- [ ] Verify: hypothesis → experiment → code generation chain works

**Why this matters:** Without code generation, experiments cannot be executed.

### 2.2 Execution Path
- [ ] Configure Docker daemon for sandboxed execution
- [ ] Enable test_sandboxed_execution
- [ ] Verify: generated code runs safely in container
- [ ] Test resource limits (memory, CPU, timeout)

**Why this matters:** Unsandboxed execution is unsafe and limits experiment types.

### 2.3 Analysis Path
- [ ] Fix test_statistical_analysis API mismatch
- [ ] Fix test_data_analyst API mismatch
- [ ] Verify: experiment results → statistical analysis → interpretation chain works

**Why this matters:** Without analysis, results cannot inform hypothesis refinement.

### 2.4 Persistence Path
- [ ] Fix Hypothesis model ID autoincrement issue
- [ ] Fix test_database_persistence
- [ ] Verify: research state survives restart

**Why this matters:** Long runs require state persistence.

### 2.5 Optional Infrastructure
- [ ] Configure Neo4j for knowledge graph (optional)
- [ ] Enable test_knowledge_graph (optional)

**Exit Criteria:** 37/39 tests passing minimum, all critical path tests green

---

## Phase 3: Extended Workflow Validation

**Status:** Not Started

**Objective:** Prove the system runs autonomously for extended periods.

### 3.1 Baseline Measurement
- [ ] Run 3-cycle workflow, capture all outputs
- [ ] Document: time per cycle, tokens per cycle, failures
- [ ] Establish baseline metrics

### 3.2 Short Extended Run (5 cycles)
- [ ] Run 5-cycle workflow with 5 tasks per cycle
- [ ] Document where workflow succeeds or stalls
- [ ] Fix state machine issues as they surface
- [ ] Verify hypothesis refinement occurs between cycles
- [ ] Measure: does quality improve across iterations?

### 3.3 Medium Extended Run (10 cycles)
- [ ] Run 10-cycle workflow with 10 tasks per cycle
- [ ] Measure convergence behavior (does it converge too early?)
- [ ] Document resource usage trends
- [ ] Identify bottlenecks (LLM latency, execution time, memory)

### 3.4 Full Paper-Spec Run (20 cycles)
- [ ] Attempt 20 cycles with 10 tasks per cycle
- [ ] Document total runtime and estimated cost
- [ ] Capture final research report
- [ ] Evaluate report quality

**Exit Criteria:** System completes 10+ cycles producing coherent output

---

## Phase 4: Model Tier Comparison

**Status:** Not Started

**Objective:** Quantify quality difference between model tiers.

### 4.1 Test Matrix

| Run | Model | Cycles | Tasks | Purpose |
|-----|-------|--------|-------|---------|
| A | Ollama/Qwen 4B | 5 | 5 | Baseline (local) |
| B | Claude Sonnet | 5 | 5 | Mid-tier comparison |
| C | GPT-5 class | 5 | 5 | High-tier comparison |

### 4.2 Quality Metrics
- [ ] Hypothesis novelty scores (system-generated)
- [ ] Hypothesis testability scores (system-generated)
- [ ] Experiment feasibility assessment
- [ ] Code generation success rate
- [ ] Execution success rate
- [ ] Manual evaluation: are outputs scientifically reasonable?

### 4.3 Cost Metrics
- [ ] Tokens per cycle per model
- [ ] USD per cycle per model
- [ ] Time per cycle per model

### 4.4 Analysis
- [ ] Document quality delta between tiers
- [ ] Determine minimum viable model tier
- [ ] Calculate cost/quality tradeoff

**Exit Criteria:** Documented comparison with recommendations

---

## Phase 5: Paper Claims Assessment

**Status:** Not Started

**Objective:** Honest measurement against paper benchmarks.

### 5.1 Accuracy Claim (79.4%)

**Problem:** Paper does not specify:
- What dataset of "scientific statements"
- Definition of "accuracy"
- Evaluation methodology

**Approach:**
- [ ] Research if benchmark dataset is publicly available
- [ ] If not, create proxy benchmark (e.g., SciQ, scientific QA datasets)
- [ ] Define accuracy metric (exact match, semantic similarity, expert rating)
- [ ] Run system against benchmark
- [ ] Report measured accuracy with methodology

### 5.2 Discovery Claim (7 validated discoveries)

**Problem:** Paper does not specify:
- What domains
- What "validated" means
- Timeframe

**Approach:**
- [ ] Run extended campaigns in specific domains (biology, physics)
- [ ] Collect all generated "discoveries"
- [ ] Define validation criteria (novel, testable, supported by evidence)
- [ ] Self-evaluate or seek domain expert review
- [ ] Report findings honestly

### 5.3 Final Documentation
- [ ] Write comparison document: implementation vs paper
- [ ] Document architectural differences
- [ ] Document performance gaps with hypotheses for why
- [ ] Publish as VALIDATION_REPORT.md

**Exit Criteria:** Honest public documentation of measured performance

---

## Success Metrics

| Metric | Paper Claim | Realistic Target | Current |
|--------|-------------|------------------|---------|
| E2E Tests | N/A | 39/39 | 32/39 |
| Max Autonomous Cycles | 20 | 10+ | 2-3 |
| Tasks per Cycle | 10 | 10 | 5 |
| Accuracy | 79.4% | Measured honestly | Unknown |
| Discoveries | 7 | Documented attempts | 0 |

---

## Risk Register

| Risk | Impact | Mitigation |
|------|--------|------------|
| Docker unavailable in test environment | Blocks Phase 2.2 | Use mock executor, document limitation |
| Production LLM costs exceed budget | Blocks Phase 4 | Use smaller run sizes, estimate before executing |
| Workflow fails at scale | Blocks Phase 3.3+ | Fix incrementally, document failure modes |
| No benchmark dataset exists | Blocks Phase 5.1 | Create proxy benchmark, document methodology |
| Results don't match paper | Reputational | Report honestly, hypothesize reasons |

---

## Time Estimates (Rough)

| Phase | Estimated Effort | Dependencies |
|-------|------------------|--------------|
| Phase 2 | 2-4 sessions | None |
| Phase 3 | 2-3 sessions | Phase 2 complete |
| Phase 4 | 1-2 sessions | Phase 3 complete, API keys funded |
| Phase 5 | 2-4 sessions | Phase 4 complete |

**Total:** 7-13 sessions to comprehensive validation

---

## Checkpoints

After each phase, create:
1. Checkpoint file with current state
2. Resume prompt for next session
3. Updated metrics in this roadmap

---

## Principles

1. **Walk before run** - Complete each phase before starting next
2. **Document failures** - Failures are data, not embarrassments
3. **Honest reporting** - The goal is truth, not proving the paper correct
4. **Incremental progress** - Small verified steps over large leaps

---

*Roadmap created: 2025-11-27*
*Last updated: 2025-11-27*
