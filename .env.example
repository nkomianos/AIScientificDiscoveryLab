# Kosmos AI Scientist Configuration
# Copy this file to .env and fill in your values

# ============================================================================
# LLM PROVIDER SELECTION
# ============================================================================

# Choose which LLM provider to use:
#   - "anthropic" (default): Direct Anthropic Claude API
#   - "openai": OpenAI API or OpenAI-compatible endpoints
#   - "litellm": LiteLLM universal interface (100+ providers)
#
# For local models (Ollama, LM Studio) or DeepSeek, use: LLM_PROVIDER=litellm
LLM_PROVIDER=anthropic

# ============================================================================
# ANTHROPIC/CLAUDE CONFIGURATION
# ============================================================================

# Option 1: Use Claude Code CLI (Recommended for Max subscription users)
# Set API key to all 9s to route to local Claude Code CLI
# Requires: pip install git+https://github.com/jimmc414/claude_n_codex_api_proxy.git
# Requires: claude CLI installed and authenticated
ANTHROPIC_API_KEY=999999999999999999999999999999999999999999999999

# Option 2: Use Anthropic API (Pay-per-use)
# Get your API key from https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-api03-your-actual-key-here

# Claude model to use (only for API mode, CLI uses Max subscription default)
# Options: claude-sonnet-4-5, claude-haiku-4-5, claude-3-5-sonnet-20241022, claude-3-opus-20240229
CLAUDE_MODEL=claude-sonnet-4-5

# Maximum tokens per request
CLAUDE_MAX_TOKENS=4096

# Temperature for generation (0.0-1.0, higher = more creative)
CLAUDE_TEMPERATURE=0.7

# Enable prompt caching to reduce API costs (true/false)
CLAUDE_ENABLE_CACHE=true

# ============================================================================
# OPENAI CONFIGURATION
# ============================================================================

# OpenAI API Configuration
# Required if LLM_PROVIDER=openai
# Get your API key from https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-proj-your-actual-key-here

# Model to use
# Options: gpt-4-turbo, gpt-4, gpt-3.5-turbo, o1-preview, o1-mini
# OPENAI_MODEL=gpt-4-turbo

# Maximum tokens per request
# OPENAI_MAX_TOKENS=4096

# Temperature (0.0-2.0, higher = more creative)
# OPENAI_TEMPERATURE=0.7

# ============================================================================
# LITELLM PROVIDER (Recommended for multi-provider support)
# ============================================================================
# LiteLLM provides a unified interface for 100+ LLM providers
# Supports: Anthropic, OpenAI, Ollama, DeepSeek, Azure, Bedrock, and more
# Docs: https://docs.litellm.ai/docs/providers

# Use LiteLLM as the provider (recommended for Ollama/DeepSeek/multi-provider)
# LLM_PROVIDER=litellm

# Model format: "provider/model" or just "model" for OpenAI
# Examples:
#   LITELLM_MODEL=ollama/llama3.1:8b          # Ollama local
#   LITELLM_MODEL=ollama/mistral              # Ollama Mistral
#   LITELLM_MODEL=deepseek/deepseek-chat      # DeepSeek
#   LITELLM_MODEL=deepseek/deepseek-coder     # DeepSeek Coder
#   LITELLM_MODEL=claude-3-5-sonnet-20241022  # Anthropic
#   LITELLM_MODEL=gpt-4-turbo                 # OpenAI
#   LITELLM_MODEL=azure/gpt-4                 # Azure OpenAI
# LITELLM_MODEL=ollama/llama3.1:8b

# API base URL (for local providers like Ollama, LM Studio)
# LITELLM_API_BASE=http://localhost:11434

# API key (optional for local providers, required for cloud)
# For DeepSeek: Get from https://platform.deepseek.com/
# LITELLM_API_KEY=sk-...

# Default generation settings
# LITELLM_MAX_TOKENS=4096
# LITELLM_TEMPERATURE=0.7
# LITELLM_TIMEOUT=120

# ============================================================================
# QUICK LITELLM EXAMPLES
# ============================================================================

# --- Example: Ollama (local, free) ---
# LLM_PROVIDER=litellm
# LITELLM_MODEL=ollama/llama3.1:8b
# LITELLM_API_BASE=http://localhost:11434

# --- Example: DeepSeek (cloud, cheap) ---
# LLM_PROVIDER=litellm
# LITELLM_MODEL=deepseek/deepseek-chat
# DEEPSEEK_API_KEY=sk-your-deepseek-key

# --- Example: LM Studio (local, free) ---
# LLM_PROVIDER=litellm
# LITELLM_MODEL=openai/local-model
# LITELLM_API_BASE=http://localhost:1234/v1

# ============================================================================
# OPENAI-COMPATIBLE PROVIDERS (Legacy - use LiteLLM instead)
# ============================================================================

# Example: Use Ollama locally (via OpenAI-compatible interface)
# LLM_PROVIDER=openai
# OPENAI_API_KEY=ollama
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_MODEL=llama3.1:70b

# Example: Use OpenRouter (access to 100+ models)
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-or-v1-your-key-here
# OPENAI_BASE_URL=https://openrouter.ai/api/v1
# OPENAI_MODEL=anthropic/claude-3.5-sonnet

# Example: Use LM Studio locally
# LLM_PROVIDER=openai
# OPENAI_API_KEY=lm-studio
# OPENAI_BASE_URL=http://localhost:1234/v1
# OPENAI_MODEL=local-model

# Custom base URL for OpenAI-compatible APIs (leave blank for official OpenAI)
# OPENAI_BASE_URL=

# OpenAI organization ID (optional, only for official OpenAI API)
# OPENAI_ORGANIZATION=

# ============================================================================
# RESEARCH CONFIGURATION
# ============================================================================

# Maximum research iterations before convergence check
MAX_RESEARCH_ITERATIONS=10

# Research budget in USD for API costs (0.0 = unlimited)
RESEARCH_BUDGET_USD=10.0

# Domains to enable
# Format: Comma-separated values OR JSON array
# Options: biology, physics, chemistry, neuroscience, materials, social_science
# Examples:
#   ENABLED_DOMAINS=biology,physics,chemistry,neuroscience
#   ENABLED_DOMAINS=["biology","physics","chemistry","neuroscience"]
ENABLED_DOMAINS=biology,physics,chemistry,neuroscience

# Experiment types to allow
# Format: Comma-separated values OR JSON array
# Options: computational, data_analysis, literature_synthesis, simulation
# Examples:
#   ENABLED_EXPERIMENT_TYPES=computational,data_analysis,literature_synthesis
#   ENABLED_EXPERIMENT_TYPES=["computational","data_analysis","literature_synthesis"]
ENABLED_EXPERIMENT_TYPES=computational,data_analysis,literature_synthesis

# Minimum novelty score for hypothesis (0.0-1.0)
MIN_NOVELTY_SCORE=0.6

# Enable autonomous iteration (true/false)
ENABLE_AUTONOMOUS_ITERATION=true

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# Database URL for SQLAlchemy
# SQLite (default, single file): sqlite:///kosmos.db
#   - Relative paths are automatically converted to absolute paths based on project root
#   - This ensures consistent database location regardless of current working directory
#   - Database will be created in the Kosmos project directory
# PostgreSQL (production, with Docker): postgresql://kosmos:kosmos-dev-password@localhost:5432/kosmos
# PostgreSQL (custom): postgresql://user:password@host:port/database
DATABASE_URL=sqlite:///kosmos.db

# Enable database echo (SQL logging for debugging)
DATABASE_ECHO=false

# ============================================================================
# REDIS CACHE (Optional)
# ============================================================================

# Enable Redis caching (requires Redis server or Docker)
# Set to true to use Redis for caching (faster than in-memory cache)
REDIS_ENABLED=false

# Redis connection URL
# Format: redis://host:port/db
# Default: redis://localhost:6379/0
# With Docker: redis://localhost:6379/0 (docker-compose handles this)
REDIS_URL=redis://localhost:6379/0

# Maximum Redis connection pool size (1-1000, default: 50)
REDIS_MAX_CONNECTIONS=50

# Socket timeout in seconds (1-60, default: 5)
REDIS_SOCKET_TIMEOUT=5

# Socket connect timeout in seconds (1-60, default: 5)
REDIS_SOCKET_CONNECT_TIMEOUT=5

# Retry on timeout (true/false, default: true)
REDIS_RETRY_ON_TIMEOUT=true

# Decode responses as UTF-8 strings (true/false, default: true)
REDIS_DECODE_RESPONSES=true

# Default cache TTL in seconds (60-86400, default: 3600 = 1 hour)
REDIS_DEFAULT_TTL_SECONDS=3600

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: json or text
LOG_FORMAT=json

# Log file path (leave empty to log to stdout only)
LOG_FILE=logs/kosmos.log

# Enable debug mode (verbose output)
DEBUG_MODE=false

# Debug verbosity level: 0=off, 1=critical path, 2=full trace, 3=data dumps
DEBUG_LEVEL=0

# Comma-separated list of modules to debug (blank = all when debug_mode=True)
# DEBUG_MODULES=research_director,workflow,orchestration

# Log LLM request/response summaries
LOG_LLM_CALLS=false

# Log inter-agent message routing
LOG_AGENT_MESSAGES=false

# Log workflow state machine transitions with timing
LOG_WORKFLOW_TRANSITIONS=false

# Enable real-time stage tracking (outputs JSON events to file)
STAGE_TRACKING_ENABLED=false
STAGE_TRACKING_FILE=logs/stages.jsonl

# ============================================================================
# LITERATURE APIS (Optional)
# ============================================================================

# Semantic Scholar API key (optional, increases rate limits)
# Get from: https://www.semanticscholar.org/product/api
# SEMANTIC_SCHOLAR_API_KEY=your-key-here

# PubMed API key (optional, increases rate limits)
# Get from: https://www.ncbi.nlm.nih.gov/account/
# PUBMED_API_KEY=your-key-here

# Email for PubMed E-utilities (recommended, helps with rate limits)
# PUBMED_EMAIL=your-email@example.com

# Literature cache TTL in hours (24-168, default: 48)
LITERATURE_CACHE_TTL_HOURS=48

# Maximum results per literature search query (1-1000, default: 100)
MAX_RESULTS_PER_QUERY=100

# PDF download timeout in seconds (5-120, default: 30)
PDF_DOWNLOAD_TIMEOUT=30

# ============================================================================
# VECTOR DATABASE (For semantic search)
# ============================================================================

# Vector DB type: chromadb, pinecone, weaviate
VECTOR_DB_TYPE=chromadb

# ChromaDB settings (local, no key needed)
CHROMA_PERSIST_DIRECTORY=.chroma_db

# Pinecone settings (if using Pinecone)
# PINECONE_API_KEY=your-key-here
# PINECONE_ENVIRONMENT=us-west1-gcp
# PINECONE_INDEX_NAME=kosmos

# ============================================================================
# NEO4J KNOWLEDGE GRAPH
# ============================================================================

# Neo4j connection URI
NEO4J_URI=bolt://localhost:7687

# Neo4j authentication
NEO4J_USER=neo4j
NEO4J_PASSWORD=kosmos-password

# Neo4j database name (default: neo4j)
NEO4J_DATABASE=neo4j

# Neo4j connection pool settings
NEO4J_MAX_CONNECTION_LIFETIME=3600
NEO4J_MAX_CONNECTION_POOL_SIZE=50

# ============================================================================
# DOMAIN-SPECIFIC APIS (Optional)
# ============================================================================

# Biology APIs
# KEGG_API_KEY=your-key-here
# UNIPROT_API_KEY=your-key-here

# Materials Science APIs
# MATERIALS_PROJECT_API_KEY=your-key-here

# Astronomy APIs
# NASA_API_KEY=your-key-here

# ============================================================================
# SAFETY CONFIGURATION
# ============================================================================

# Enable code safety checks
ENABLE_SAFETY_CHECKS=true

# Maximum execution time for experiments (seconds)
MAX_EXPERIMENT_EXECUTION_TIME=300

# Maximum memory usage (MB)
MAX_MEMORY_MB=2048

# Enable sandboxing for code execution
ENABLE_SANDBOXING=true

# Require human approval for high-risk operations
REQUIRE_HUMAN_APPROVAL=false

# ============================================================================
# PERFORMANCE CONFIGURATION
# ============================================================================

# Enable result caching
ENABLE_RESULT_CACHING=true

# Cache TTL (seconds)
CACHE_TTL=3600

# Number of parallel experiments (0 = sequential, default: 0)
# Recommended: CPU cores - 1 (e.g., 3 for 4-core, 7 for 8-core)
PARALLEL_EXPERIMENTS=0

# ============================================================================
# CONCURRENT OPERATIONS CONFIGURATION
# ============================================================================

# Enable concurrent research operations (true/false, default: false)
# When enabled, performs hypothesis evaluation, experiment execution,
# and result analysis concurrently for significant performance gains
# Expected speedup: 2-4Ã— faster research cycles
# Note: Requires ParallelExperimentExecutor and AsyncClaudeClient
ENABLE_CONCURRENT_OPERATIONS=false

# Maximum concurrent experiment executions (1-16, default: 4)
# Number of experiments to run in parallel
# Recommended: CPU cores - 1 (e.g., 3 for 4-core, 7 for 8-core)
# Higher values increase throughput but consume more CPU/memory
MAX_CONCURRENT_EXPERIMENTS=4

# Maximum concurrent hypothesis evaluations (1-10, default: 3)
# Number of hypotheses to evaluate in parallel using async LLM calls
# Higher values = faster evaluation but may hit rate limits
MAX_PARALLEL_HYPOTHESES=3

# ============================================================================
# ASYNC LLM CONFIGURATION (For concurrent API calls)
# ============================================================================

# Maximum concurrent LLM API calls (1-20, default: 5)
# Controls semaphore limit for async Claude API requests
# Higher values = faster but may hit rate limits
# Anthropic recommended: 5 for regular, 10 for rate limit increase
MAX_CONCURRENT_LLM_CALLS=5

# LLM rate limit per minute (1-200, default: 50)
# Token bucket rate limiting for API calls
# Set based on your Anthropic API tier limits
# Tier 1: 50/min, Tier 2: 100/min, Tier 3: 200/min
LLM_RATE_LIMIT_PER_MINUTE=50

# Async batch operation timeout in seconds (10-3600, default: 300)
# Timeout for concurrent batch operations (hypothesis eval, result analysis)
# Increase if dealing with complex operations or slow API responses
ASYNC_BATCH_TIMEOUT=300

# Legacy settings (for backward compatibility)
MAX_PARALLEL_HYPOTHESIS_EVALUATIONS=3
ENABLE_CONCURRENT_RESULT_ANALYSIS=true

# ============================================================================
# PROFILING CONFIGURATION (For performance analysis)
# ============================================================================

# Enable performance profiling (true/false, default: false)
# Note: Profiling adds ~5-10% overhead in full mode
ENABLE_PROFILING=false

# Profiling mode: light, standard, full
# - light: Basic timing + memory (< 1% overhead)
# - standard: + cProfile for CPU profiling (~5% overhead)
# - full: + line-by-line profiling (~10-15% overhead)
PROFILING_MODE=light

# Store profile results in database (true/false, default: true)
STORE_PROFILE_RESULTS=true

# Profile data retention in days (1-90, default: 30)
PROFILE_STORAGE_DAYS=30

# Enable automatic bottleneck detection (true/false, default: true)
ENABLE_BOTTLENECK_DETECTION=true

# Bottleneck threshold percentage (1-50, default: 10)
# Functions taking > X% of total time are flagged
BOTTLENECK_THRESHOLD_PERCENT=10

# ============================================================================
# MONITORING & METRICS
# ============================================================================

# Enable usage statistics tracking
ENABLE_USAGE_STATS=true

# Metrics export interval (seconds, 0 = disabled)
METRICS_EXPORT_INTERVAL=60

# ============================================================================
# DEVELOPMENT SETTINGS
# ============================================================================

# Enable hot reload (development only)
HOT_RELOAD=false

# Enable API request logging
LOG_API_REQUESTS=false

# Test mode (uses mocks instead of real APIs)
TEST_MODE=false
